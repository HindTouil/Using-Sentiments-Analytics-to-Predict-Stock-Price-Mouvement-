{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGMB with tuned parameters, using 2 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "50bc29d151fa6192be326cbc343ce0b6e40405b4"
   },
   "source": [
    "### Importer les packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from kaggle.competitions import twosigmanews\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime, date\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "### Récupérer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a60d768ec157469fe0512b86356dace3c41233a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = twosigmanews.make_env()\n",
    "\n",
    "(market_train_df, news_train_df) = env.get_training_data()\n",
    "market_train, news_train = market_train_df.copy(), news_train_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51d904e9fd57dc968c9808e0f86c43d553afc5ed"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre cas le volume des données utilisées influence l'apprentissage de notre modèle, appliquer un dropna() va supprimer un grand nombre de lignes, donc nous allons les remplir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fonction pour remplir les données nulles\n",
    "def remp_val_nul(dataframe):\n",
    "    for i in dataframe.columns:\n",
    "        #categorical value\n",
    "        if dataframe[i].dtype == \"object\":\n",
    "            dataframe[i] = dataframe[i].fillna(\"other\")\n",
    "            #numerical value\n",
    "        elif (dataframe[i].dtype == \"int64\" or dataframe[i].dtype == \"float64\"):\n",
    "            dataframe[i] = dataframe[i].fillna(dataframe[i].mean())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "market_train_df = remp-val_nul(market_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir prendre en considération les assetCodes pour l'apprentissage de notre modèle, il faut les transformer en valeurs numériques discrètes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fonction ayant même role que label encoder\n",
    "def prepro(market_train):\n",
    "    market_train.time = market_train.time.dt.date\n",
    "    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n",
    "    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n",
    "    market_train = market_train.dropna(axis=0)\n",
    "    return market_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4856c0a87ba664a6040bcda5b90caa15006b4b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "market_train = prepro(market_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données avant 2009 présentent des anomalies, donc nous allons prendre en considération les données dont la date est supérieure que 20009/01/01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "market_train = market_train.loc[market_train['time']>=date(2009, 1, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dict ayant les colonnes de news dataframe et les fcts d'agg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_cols_agg = {\n",
    "    'urgency': ['min', 'count'],\n",
    "    'takeSequence': ['max'],\n",
    "    'bodySize': ['min', 'max', 'mean', 'std'],\n",
    "    'wordCount': ['min', 'max', 'mean', 'std'],\n",
    "    'sentenceCount': ['min', 'max', 'mean', 'std'],\n",
    "    'companyCount': ['min', 'max', 'mean', 'std'],\n",
    "    'marketCommentary': ['min', 'max', 'mean', 'std'],\n",
    "    'relevance': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n",
    "    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n",
    "    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n",
    "    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction pour joindre les données de Market et News en un deul dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_market_news(market_train_df, news_train_df):\n",
    "    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")\n",
    "    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n",
    "    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n",
    "\n",
    "    assert len(assetCodes_index) == len(assetCodes_expanded)\n",
    "    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n",
    "    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n",
    "    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n",
    "    del news_train_df, df_assetCodes\n",
    "    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n",
    "    del news_train_df_expanded\n",
    "    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n",
    "    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n",
    "    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n",
    "    del news_train_df_aggregated\n",
    "    \n",
    "    return market_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Récupérer X et Y\n",
    "def get_xy(market_train_df, news_train_df, le=None):\n",
    "    x, le = get_x(market_train_df, news_train_df)\n",
    "    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n",
    "    return x, y, le\n",
    "\n",
    "\n",
    "def label_encode(series, min_count):\n",
    "    vc = series.value_counts()\n",
    "    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n",
    "    return le\n",
    "\n",
    "\n",
    "def get_x(market_train_df, news_train_df, le=None):\n",
    "    #En bourse l'ouverture et cloture se font à 22h, pour pouvoir faiire la différence entre les différentes journées, nous alons\n",
    "    #considérer qu'après 22h une nouvelle journée commence\n",
    "    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n",
    "    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n",
    "\n",
    "    # Join market and news\n",
    "    x = join_market_news(market_train_df, news_train_df)\n",
    "    #encode assetCode and assetname\n",
    "    if le is None:\n",
    "        le_assetCode = label_encode(x['assetCode'], min_count=10)\n",
    "        le_assetName = label_encode(x['assetName'], min_count=5)\n",
    "    else:\n",
    "        le_assetCode, le_assetName = le  \n",
    "    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n",
    "    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n",
    "    \n",
    "    try:\n",
    "        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        x.drop(columns=['universe'], inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n",
    "    x.drop(columns='time', inplace=True)\n",
    "    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n",
    "        x[bogus_col] = x[bogus_col].astype(float)\n",
    "    \n",
    "    return x, (le_assetCode, le_assetName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, le = get_xy(market_train_df, news_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "886507a9c0384f0919b922b5ff8e2764c9575e0c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scaling of X values\n",
    "mins = np.min(X, axis=0)\n",
    "maxs = np.max(X, axis=0)\n",
    "rng = maxs - mins\n",
    "X = 1 - ((maxs - X) / rng)\n",
    "\n",
    "\n",
    "X_train, X_test,  r_train, r_test = model_selection.train_test_split(X,  r, test_size=0.25, random_state=99)\n",
    "\n",
    "dtrain = lgb.Dataset(X_train.values, y_train, free_raw_data=False)\n",
    "dvalid = lgb.Dataset(X_valid.values, y_valid, free_raw_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45c99e6b4fb86f54d020723d13031a923c8d3808"
   },
   "source": [
    "Training pour deux modèles dont x_1 et x_2 proviennent de deux différents parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b6ac689319157951828d4e4a713ae688ea5e618c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_1 = [0.19000424246380565, 2452, 212, 328, 202]\n",
    "x_2 = [0.19016805202090095, 2583, 213, 312, 220]\n",
    "\n",
    "params_1 = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'dart',\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': x_1[0],\n",
    "        'num_leaves': x_1[1],\n",
    "        'min_data_in_leaf': x_1[2],\n",
    "        'num_iteration': x_1[3],\n",
    "        'max_bin': x_1[4],\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "params_2 = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'dart',\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': x_2[0],\n",
    "        'num_leaves': x_2[1],\n",
    "        'min_data_in_leaf': x_2[2],\n",
    "        'num_iteration': x_2[3],\n",
    "        'max_bin': x_2[4],\n",
    "        'verbose': 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm_1 = lgb.train(params_1,\n",
    "        dtrain,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=dvalid,\n",
    "        early_stopping_rounds=5)\n",
    "        \n",
    "gbm_2 = lgb.train(params_2,\n",
    "        dtrain,\n",
    "        num_boost_round=100,\n",
    "        valid_sets=dvalid,\n",
    "        early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire la prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(predictions_template_df, market_obs_df, news_obs_df, le):\n",
    "    x, _ = get_x(market_obs_df, news_obs_df, le)\n",
    "    predictions_template_df.confidenceValue = np.clip((gbm_1.predict(x) + gbm_2.predict(x))/2, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7376622bed44ed4f009c47ed1d5acf177720f857",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()\n",
    "\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    make_predictions(predictions_template_df, market_obs_df, news_obs_df, le)\n",
    "    print(predictions_template_df)\n",
    "    env.predict(predictions_template_df)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "env.write_submission_file()\n",
    "sub  = pd.read_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
